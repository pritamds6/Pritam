{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOeVMZmtSA07keJ0+C+WkiW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pritamds6/Pritam/blob/master/Scratch_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4S3RdwMfNzcy"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.nn import Linear, Module, Parameter, Sequential\n",
        "\n",
        "EPS = torch.finfo(torch.float32).eps\n",
        "\n",
        "\n",
        "class CosinePositionalEncoding(Module):\n",
        "    def __init__(self, seq_len: int, dim_emb: int, base: int = 10_000, eps: float = EPS) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        indices = torch.arange(0, seq_len, dtype=torch.float)\n",
        "        scale = 1 / (base ** (torch.arange(0, dim_emb, 2, dtype=torch.float) / dim_emb) + eps)\n",
        "\n",
        "        position = torch.zeros(1, 1, seq_len, dim_emb)\n",
        "        position[:, :, :, 0::2] = torch.sin(indices[None, None, :, None] * scale)\n",
        "        position[:, :, :, 1::2] = torch.cos(indices[None, None, :, None] * scale)\n",
        "\n",
        "        self.register_buffer(\"position\", position)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return x + self.position  # (bs, num_heads, seq_len, dim_emb)\n",
        "\n",
        "\n",
        "class RotaryPositionalEncoding(Module):\n",
        "    def __init__(self, seq_len: int, dim_emb: int, base: int = 10000, eps: float = EPS) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim_emb = dim_emb\n",
        "        indices = torch.arange(0, seq_len, dtype=torch.float)\n",
        "        scale = 1 / (base ** (torch.arange(0, dim_emb, 2, dtype=torch.float) / dim_emb) + eps)\n",
        "\n",
        "        position = torch.outer(indices, scale)\n",
        "        position = torch.cat((position, position), dim=-1)\n",
        "\n",
        "        position_cos = torch.cos(position[None, None, :, :])  # (bs, num_heads, seq_len, dim_emb)\n",
        "        position_sin = torch.sin(position[None, None, :, :])  # (bs, num_heads, seq_len, dim_emb)\n",
        "\n",
        "        self.register_buffer(\"position_cos\", position_cos)\n",
        "        self.register_buffer(\"position_sin\", position_sin)\n",
        "\n",
        "    def _rotate_half(self, x: Tensor) -> Tensor:\n",
        "        x1, x2 = x[..., : self.dim_emb // 2], x[..., self.dim_emb // 2 :]\n",
        "        return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # x is of shape  (bs, num_heads, seq_len, dim_emb)\n",
        "        return (x * self.position_cos) + (self._rotate_half(x) * self.position_sin)\n",
        "\n",
        "\n",
        "class RMSNorm(Module):\n",
        "    # RMSnorm(x_i) = (x_i / RMS(x)) * g_i where RMS(x) = sqrt(1 / n *  sum a_i ** 2)\n",
        "    def __init__(self, dim_last: int, eps: float = EPS):\n",
        "        super().__init__()\n",
        "\n",
        "        self.eps = eps\n",
        "        self.gain = Parameter(torch.ones(dim_last), requires_grad=True)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        scale = torch.rsqrt(torch.mean(x * x, dim=-1, keepdim=True) + self.eps)\n",
        "        return x * scale * self.gain\n",
        "\n",
        "\n",
        "class SwiGLU(Module):\n",
        "    # SwiGLU(x) = (xW + b) ⊗ swish(xZ + c) where W, Z, b, c are learnable params\n",
        "    def __init__(self, dim_in: int, bias: bool = True) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim_in = dim_in\n",
        "        self.linear = Linear(dim_in, 2 * dim_in, bias=bias)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # uses only one weight matrix instead of two\n",
        "        out = self.linear(x)\n",
        "        return F.silu(out[..., : self.dim_in]) + out[..., self.dim_in :]\n",
        "\n",
        "\n",
        "class SelfAttention(Module):\n",
        "    def __init__(self, seq_len: int, dim_emb: int, dim_k: int = None, dim_v: int = None, causal=True) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim_k = dim_k or dim_emb\n",
        "        self.dim_v = dim_v or dim_emb\n",
        "        self.causal = causal\n",
        "\n",
        "        # Query, Key and Value projections\n",
        "        self.proj_q = Linear(dim_emb, self.dim_k, bias=False)\n",
        "        self.proj_k = Linear(dim_emb, self.dim_k, bias=False)\n",
        "        self.proj_v = Linear(dim_emb, self.dim_v, bias=False)\n",
        "        self.proj_out = Linear(self.dim_v, self.dim_v, bias=False)\n",
        "\n",
        "        # Build the causal mask, masking upper triangular part of attention scores\n",
        "        self.register_buffer(\"causal_mask\", torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool())\n",
        "\n",
        "    def forward(self, x: Tensor, return_scores: bool = False) -> Tensor | Tuple[Tensor, Tensor]:\n",
        "        # projects input to Q, K, V spaces\n",
        "        q = self.proj_q(x)  # (bs, seq_len, dim_k)\n",
        "        k = self.proj_k(x)  # (bs, seq_len, dim_k)\n",
        "        v = self.proj_v(x)  # (bs, seq_len, dim_v)\n",
        "\n",
        "        # Compute the correlation between a query q_i and all the keys, for every q_i\n",
        "        attn_scores = q @ torch.transpose(k, 2, 1)  # (bs, seq_len, seq_len)\n",
        "\n",
        "        # Fill the upper triangular part of the attention scores with -inf to inhibit them in the softmax\n",
        "        if self.causal:\n",
        "            attn_scores.masked_fill_(self.causal_mask[None, ...], -torch.inf)\n",
        "\n",
        "        attn_scores = torch.softmax(attn_scores * self.dim_k**-0.5, dim=-1)  # (bs, seq_len, seq_len)\n",
        "        out = self.proj_out(attn_scores @ v)  # (bs, seq_len, dim_v)\n",
        "\n",
        "        if return_scores:\n",
        "            return out, attn_scores\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(Module):\n",
        "    def __init__(\n",
        "        self, seq_len: int, num_heads: int, dim_emb: int, dim_k: int = None, dim_v: int = None, causal=True\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        assert dim_emb % num_heads == 0, \"num_heads must be a multiple of dim_emb\"\n",
        "\n",
        "        self.seq_len = seq_len\n",
        "        self.num_heads = num_heads\n",
        "        self.dim_head = dim_emb // num_heads\n",
        "        self.dim_k = dim_k or dim_emb\n",
        "        self.dim_v = dim_v or dim_emb\n",
        "        self.causal = causal\n",
        "\n",
        "        # positional encoding to be applied to query and key projections\n",
        "        # self.positional_encoding = CosinePositionalEncoding(seq_len, dim_emb // num_heads)\n",
        "        self.positional_encoding = RotaryPositionalEncoding(seq_len, dim_emb // num_heads)\n",
        "\n",
        "        # Query, Key and Value projections\n",
        "        self.proj_q = Linear(dim_emb, self.dim_k, bias=False)\n",
        "        self.proj_k = Linear(dim_emb, self.dim_k, bias=False)\n",
        "        self.proj_v = Linear(dim_emb, self.dim_v, bias=False)\n",
        "        self.proj_out = Linear(self.dim_v, self.dim_v, bias=False)\n",
        "\n",
        "        # Build the causal mask, masking upper triangular part of attention scores\n",
        "        self.register_buffer(\"causal_mask\", torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool())\n",
        "\n",
        "    def forward(self, x: Tensor, return_scores: bool = False) -> Tensor | Tuple[Tensor, Tensor]:\n",
        "        # projects input to Q, K, V spaces\n",
        "        q = self.proj_q(x)  # (bs, seq_len, dim_k)\n",
        "        k = self.proj_k(x)  # (bs, seq_len, dim_k)\n",
        "        v = self.proj_v(x)  # (bs, seq_len, dim_v)\n",
        "\n",
        "        # split projections between heads -> (bs, num_heads, seq_len, dim_k)\n",
        "        q = q.view(-1, self.seq_len, self.num_heads, self.dim_head).permute(0, 2, 1, 3)\n",
        "        k = k.view(-1, self.seq_len, self.num_heads, self.dim_head).permute(0, 2, 1, 3)\n",
        "        v = v.view(-1, self.seq_len, self.num_heads, self.dim_head).permute(0, 2, 1, 3)\n",
        "\n",
        "        # apply positional encoding to projections, for each heads\n",
        "        q = self.positional_encoding(q)  # (bs, num_heads, seq_len, dim_k)\n",
        "        k = self.positional_encoding(k)  # (bs, num_heads, seq_len, dim_k)\n",
        "\n",
        "        # Compute the correlation between a query q_i and all the keys, for every q_i\n",
        "        attn_scores = (q @ k.permute(0, 1, 3, 2)) * self.dim_k**-0.5  # (bs, num_heads, seq_len, seq_len)\n",
        "\n",
        "        # Fill the upper triangular part of the attention scores with -inf to inhibit them in the softmax\n",
        "        if self.causal:\n",
        "            attn_scores.masked_fill_(self.causal_mask[None, None, ...], -torch.inf)\n",
        "\n",
        "        # attention scores are used to build a weighted linear combination of values vectors\n",
        "        attn_scores = torch.softmax(attn_scores, dim=-1)  # (bs, num_heads, seq_len, seq_len)\n",
        "        out = attn_scores @ v  # (bs, num_heads, seq_len, dim_v)\n",
        "\n",
        "        # merge heads\n",
        "        out = out.permute(0, 2, 1, 3).contiguous().view(-1, self.seq_len, self.dim_v)  # (bs, seq_len, dim_v)\n",
        "\n",
        "        # projects to the output space\n",
        "        out = self.proj_out(out)  # (bs, seq_len, dim_v)\n",
        "\n",
        "        if return_scores:\n",
        "            return out, attn_scores\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "\n",
        "class FeedForward(Sequential):\n",
        "    def __init__(self, dim_in: int, dim_hidden: int, bias: bool = False) -> None:\n",
        "        super().__init__(\n",
        "            Linear(dim_in, dim_hidden, bias=bias),\n",
        "            SwiGLU(dim_hidden),\n",
        "            Linear(dim_hidden, dim_in, bias=bias),\n",
        "        )\n",
        "\n",
        "\n",
        "class TransformerBlock(Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        seq_len: int,\n",
        "        dim_emb: int,\n",
        "        attn_num_heads: int,\n",
        "        ffn_hidden_dim: int,\n",
        "        ffn_bias: bool = False,\n",
        "        attn_causal: bool = True,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        # Follows LLama 2 architecture:\n",
        "        # - positional encoding on every head of the multi-head attention query and keys projections\n",
        "        # - RMS pre-normalization instead of layer normalization\n",
        "        # - SwiGLU activation for the feedforward\n",
        "        self.norm_attn = RMSNorm(dim_emb)\n",
        "        self.multihead_attn = MultiHeadAttention(seq_len, attn_num_heads, dim_emb, causal=attn_causal)\n",
        "        self.norm_ffn = RMSNorm(dim_emb)\n",
        "        self.feed_forward = FeedForward(dim_emb, ffn_hidden_dim, bias=ffn_bias)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = x + self.multihead_attn(self.norm_attn(x))  # (bs, seq_len, dim_in)\n",
        "        x = x + self.feed_forward(self.norm_ffn(x))  # (bs, seq_len, dim_in)\n",
        "\n",
        "        return x  # (bs, seq_len, dim_in)"
      ],
      "metadata": {
        "id": "OPQWgsSaO9xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from sentencepiece import SentencePieceProcessor, SentencePieceTrainer\n",
        "from torch import IntTensor, Tensor\n",
        "\n",
        "__model_types = [\"unigram\", \"bpe\", \"word\", \"char\"]\n",
        "\n",
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self, path: str = None) -> None:\n",
        "        self.sp = SentencePieceProcessor()\n",
        "        self.sp.Load(model_file=path)\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self) -> int:\n",
        "        return self.sp.vocab_size()\n",
        "\n",
        "    @property\n",
        "    def bos_id(self) -> int:\n",
        "        return self.sp.bos_id()\n",
        "\n",
        "    @property\n",
        "    def eos_id(self) -> int:\n",
        "        return self.sp.eos_id()\n",
        "\n",
        "    @property\n",
        "    def pad_id(self) -> int:\n",
        "        return self.sp.pad_id()\n",
        "\n",
        "    @property\n",
        "    def unk_id(self) -> int:\n",
        "        return self.sp.unk_id()\n",
        "\n",
        "    def encode(\n",
        "        self,\n",
        "        input: str,\n",
        "        beg_of_string: bool = False,\n",
        "        end_of_string: bool = False,\n",
        "        pad_seq: bool = False,\n",
        "        seq_len: int = None,\n",
        "    ) -> Tensor:\n",
        "        out = self.sp.EncodeAsIds(input, add_bos=beg_of_string, add_eos=end_of_string)\n",
        "\n",
        "        if pad_seq and len(out) < seq_len:\n",
        "            out = [*[self.pad_id] * (seq_len - len(out)), *out]\n",
        "\n",
        "        return IntTensor(out)\n",
        "\n",
        "    def decode(self, input: Tensor) -> str:\n",
        "        out = \"\".join(self.sp.Decode(input.tolist()))\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def train_tokenizer(\n",
        "    input_file: str,\n",
        "    vocab_size: int,\n",
        "    pad_id: int = 0,\n",
        "    unk_id: int = 1,\n",
        "    bod_id: int = 2,\n",
        "    eos_id: int = 3,\n",
        "    model_type: str = \"unigram\",\n",
        "    max_sample_size: int = 1_000_000,\n",
        ") -> None:\n",
        "    assert model_type in __model_types, f\"Got invalid model_type argument: {model_type}\"\n",
        "    SentencePieceTrainer.Train(\n",
        "        input=input_file,\n",
        "        vocab_size=vocab_size,\n",
        "        model_type=model_type,\n",
        "        model_prefix=Path(input_file).with_suffix(\"\"),\n",
        "        pad_id=pad_id,\n",
        "        unk_id=unk_id,\n",
        "        bos_id=bod_id,\n",
        "        eos_id=eos_id,\n",
        "        input_sentence_size=max_sample_size,\n",
        "    )"
      ],
      "metadata": {
        "id": "y0wWSCeMPEuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.nn import Dropout, Embedding, Linear, Module, Sequential\n",
        "\n",
        "#from model.transformer import RMSNorm, TransformerBlock\n",
        "\n",
        "\n",
        "class LLM(Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        context_size: int,\n",
        "        dim_emb: int,\n",
        "        num_layers: int,\n",
        "        attn_num_heads: int,\n",
        "        ffd_hidden_dim: int,\n",
        "        emb_dropout: float = 0.0,\n",
        "        ffd_bias: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.context_size = context_size\n",
        "        self.token_embedding = Embedding(vocab_size, dim_emb)\n",
        "        self.emb_dropout = Dropout(emb_dropout)\n",
        "        self.transformer = Sequential()\n",
        "\n",
        "        for _ in range(num_layers):\n",
        "            self.transformer.append(TransformerBlock(context_size, dim_emb, attn_num_heads, ffd_hidden_dim, ffd_bias))\n",
        "\n",
        "        self.norm = RMSNorm(dim_emb)\n",
        "        self.projection_head = Linear(dim_emb, vocab_size)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.token_embedding(x)  # (bs, seq_len, dim_emb)\n",
        "        x = self.emb_dropout(x)  # (bs, seq_len, dim_emb)\n",
        "        x = self.transformer(x)  # (bs, seq_len, dim_emb)\n",
        "        x = self.norm(x)  # (bs, seq_len, dim_emb)\n",
        "        x = self.projection_head(x)  # (bs, seq_len, vocab_size)\n",
        "\n",
        "        return x  # (bs, seq_len, vocab_size)\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def generate(self, inputs: Tensor, max_seq_len: int, temperature: float = 1.0, top_p: int = None) -> Tensor:\n",
        "        for _ in range(max_seq_len):\n",
        "            # make sure the sequence we're generating doesn't exceed model's sequence length\n",
        "            inputs_cond = inputs if inputs.size(1) <= self.context_size else inputs[:, -self.context_size :]\n",
        "\n",
        "            # get logits for the last sequence only, and rescale them to get a probability distribution over the vocabulary\n",
        "            logits = self(inputs_cond)[:, -1, :]  # (bs, vocab_size)\n",
        "\n",
        "            # TODO: Top-p sampling (nucleus)\n",
        "            probs = F.softmax(logits / temperature, dim=-1)  # (bs, vocab_size)\n",
        "\n",
        "            # sample the next token index\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # append to the sequence being generated\n",
        "            inputs = torch.cat((inputs, next_token), dim=-1)\n",
        "\n",
        "        return inputs"
      ],
      "metadata": {
        "id": "27XXYZtVPJ1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
        "        device = torch.device(\"mps\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    return device\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class LLMConfig:\n",
        "    vocab_size: int = 1024\n",
        "    context_size: int = 128\n",
        "    dim_emb: int = 512\n",
        "    num_layers: int = 8\n",
        "    num_heads: int = 8\n",
        "    emb_dropout: float = 0.0\n",
        "    ffd_dim_hidden: int = 4 * 512\n",
        "    ffd_bias: bool = True\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    retrain_tokenizer: bool = False\n",
        "    device: torch.device = get_device()\n",
        "    batch_size: int = 64\n",
        "    learning_rate: float = 1e-4\n",
        "    weight_decay: float = 1e-2\n",
        "    max_steps: int = 1000\n",
        "    log_frequency: int = 10"
      ],
      "metadata": {
        "id": "L9zTKogEPZcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "\n",
        "#from model.tokenizer import Tokenizer\n",
        "\n",
        "\n",
        "class NextTokenPredictionDataset:\n",
        "    def __init__(self, input_file: str, context_size: int, tokenizer: Tokenizer) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_file = input_file\n",
        "        self.context_size = context_size\n",
        "\n",
        "        # load data in memory\n",
        "        with open(self.input_file) as f:\n",
        "            data = f.read()\n",
        "\n",
        "        self.data = tokenizer.encode(data)\n",
        "\n",
        "    def get_batch(self, batch_size: int) -> tuple[Tensor]:\n",
        "        # sample random starting index in the data and build a batch from there\n",
        "        indices = torch.randint(self.data.size(0) - self.context_size, (batch_size,))\n",
        "        inputs = torch.stack([self.data[i : i + self.context_size] for i in indices], dim=0)\n",
        "        labels = torch.stack([self.data[i + 1 : i + 1 + self.context_size] for i in indices], dim=0).long()\n",
        "\n",
        "        return inputs, labels"
      ],
      "metadata": {
        "id": "jKMJB1KUPq9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Module\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#from helpers.dataset import NextTokenPredictionDataset\n",
        "\n",
        "\n",
        "def log(step, max_steps, lr, metrics, mode=\"train\"):\n",
        "    metrics_print = \" - \".join([f\"{m}: {v[-1]:.3f}\" for m, v in metrics.items()])\n",
        "\n",
        "    if mode == \"train\":\n",
        "        print(f\"Step {step + 1}/{max_steps} - LR:{lr:.4f} -\", metrics_print, end=\"\\r\")\n",
        "    if mode == \"eval\":\n",
        "        print(f\"\\n\\Step {step + 1}/{max_steps} -\", metrics_print)\n",
        "\n",
        "\n",
        "def train(\n",
        "    model: Module,\n",
        "    ds_train: NextTokenPredictionDataset,\n",
        "    device: torch.device,\n",
        "    batch_size: int,\n",
        "    lr: float,\n",
        "    max_steps: int,\n",
        "    weight_decay: float = 1e-2,\n",
        "    log_every: int = 10,\n",
        ") -> defaultdict:\n",
        "    print(f\"Training on {device}.\")\n",
        "\n",
        "    metrics_tracker = defaultdict(list)\n",
        "    model.to(device)\n",
        "    optimizer = Adam(model.parameters(), lr=10 * lr, weight_decay=weight_decay)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=max_steps, eta_min=lr)\n",
        "    model.train()\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        inputs, labels = ds_train.get_batch(batch_size)\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        logits = model(inputs)\n",
        "\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=-1)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        metrics_tracker[\"train_loss\"].append(loss.detach().cpu().item())\n",
        "        if step % log_every == 0 or step == max_steps - 1:\n",
        "            log(step, max_steps, scheduler.get_last_lr()[-1], metrics_tracker)\n",
        "\n",
        "    return metrics_tracker\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "def evaluate(model: Module, dl_val: DataLoader, device: torch.device) -> float:\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    num_steps = 0\n",
        "\n",
        "    for sequence, labels in dl_val:\n",
        "        sequence, labels = sequence.to(device), labels.to(device)\n",
        "        logits = model(sequence)\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=-1)\n",
        "\n",
        "        running_loss += loss.cpu().item()\n",
        "        num_steps += 1\n",
        "\n",
        "    return running_loss / num_steps"
      ],
      "metadata": {
        "id": "4wkONSrtPyqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "\n",
        "print(f\"pytorch version: {torch.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdMrN-x2P8Tc",
        "outputId": "1a9409c7-e6bf-49a2-f406-b4fcb0e128fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pytorch version: 2.2.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_config = LLMConfig(\n",
        "    vocab_size=2_000,\n",
        "    context_size=64,\n",
        "    dim_emb=256,\n",
        "    num_layers=4,\n",
        "    num_heads=8,\n",
        "    emb_dropout=0.0,\n",
        "    ffd_dim_hidden=4 * 256,\n",
        "    ffd_bias=False,\n",
        ")\n",
        "\n",
        "train_config = TrainingConfig(\n",
        "    retrain_tokenizer=False,\n",
        "    device=get_device(),\n",
        "    batch_size=64,\n",
        "    learning_rate=3e-4,\n",
        "    weight_decay=1e-5,\n",
        "    max_steps=4_000,\n",
        "    log_frequency=1,\n",
        ")"
      ],
      "metadata": {
        "id": "T2HRifT1QECt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "input_file = \"input.txt\"\n",
        "output_file = Path(input_file).with_suffix(\".model\")\n",
        "\n",
        "if not output_file.exists() or train_config.retrain_tokenizer:\n",
        "    train_tokenizer(input_file, llm_config.vocab_size)\n",
        "\n",
        "tokenizer = Tokenizer(str(output_file))"
      ],
      "metadata": {
        "id": "Dgghh1cKQP9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = (\n",
        "    \"The role of the tokenizer is to build a mapping between a sentences represented as a string and token indices\"\n",
        ")\n",
        "print(tokenizer.sp.EncodeAsPieces(sentence))\n",
        "\n",
        "assert tokenizer.decode(tokenizer.encode(sentence)) == sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUwQuP_yRt1x",
        "outputId": "5bdc8490-7d1a-470a-8a15-05805156b95d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁The', '▁', 'ro', 'le', '▁of', '▁the', '▁to', 'k', 'en', 'ize', 'r', '▁is', '▁to', '▁bu', 'il', 'd', '▁a', '▁m', 'app', 'ing', '▁between', '▁a', '▁sentence', 's', '▁re', 'p', 're', 's', 'ent', 'ed', '▁as', '▁a', '▁str', 'ing', '▁and', '▁to', 'k', 'en', '▁in', 'd', 'ice', 's']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train = NextTokenPredictionDataset(input_file, llm_config.context_size, tokenizer)\n",
        "\n",
        "X, y = ds_train.get_batch(batch_size=1)\n",
        "\n",
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYXmTxANRykT",
        "outputId": "97f3cdbd-5889-43d5-87b3-b9b07f06d89a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 64]) torch.Size([1, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = LLM(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    context_size=llm_config.context_size,\n",
        "    dim_emb=llm_config.dim_emb,\n",
        "    num_layers=llm_config.num_layers,\n",
        "    attn_num_heads=llm_config.num_heads,\n",
        "    emb_dropout=llm_config.emb_dropout,\n",
        "    ffd_hidden_dim=llm_config.ffd_dim_hidden,\n",
        "    ffd_bias=llm_config.ffd_bias,\n",
        ")\n",
        "\n",
        "params_size = sum(p.nelement() * p.element_size() for p in model.parameters())\n",
        "buffer_size = sum(p.nelement() * p.element_size() for p in model.buffers())\n",
        "size = (params_size + buffer_size) / 1024**2\n",
        "\n",
        "print(f\"total params: {sum(p.numel() for p in model.parameters()):,d}\")\n",
        "print(f\"model size: {size:.3f}MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uChWJo6UR8LN",
        "outputId": "ee5b0c61-9743-4d86-c55a-9e43897706b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total params: 12,570,832\n",
            "model size: 48.032MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_history = train(\n",
        "    model,\n",
        "    ds_train,\n",
        "    train_config.device,\n",
        "    batch_size=train_config.batch_size,\n",
        "    lr=train_config.learning_rate,\n",
        "    max_steps=train_config.max_steps,\n",
        "    weight_decay=train_config.weight_decay,\n",
        "    log_every=train_config.log_frequency,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pv7tjJscSAXo",
        "outputId": "a21e990c-f6cf-4e50-b217-bf31b0c847c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on cuda.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(12, 4))\n",
        "ax.plot(range(len(loss_history[\"train_loss\"])), loss_history[\"train_loss\"])\n",
        "ax.set_xlabel(\"step\")\n",
        "ax.set_ylabel(\"cross entropy loss\")\n",
        "ax.grid(axis=\"y\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "YG5WL2ohaQR3",
        "outputId": "8a969823-e1bf-4a68-9b7e-927d577c2c12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA9wAAAF1CAYAAAAa3OISAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABQv0lEQVR4nO3deXxU5d3///fsWSchZIcQdpBVFsGIWwVBShW1t1qkLS63/alYF9QqbV1wKWi/pdaluG+3tdiiaLVoWRRQisi+yC5bgIQQQjJZJ7Oc3x+R0TFsA5mZJPN6Ph55OOeaMyefycUkvs91neuYDMMwBAAAAAAAmpQ52gUAAAAAANAaEbgBAAAAAAgDAjcAAAAAAGFA4AYAAAAAIAwI3AAAAAAAhAGBGwAAAACAMCBwAwAAAAAQBgRuAAAAAADCgMANAAAAAEAYELgBAAAAAAgDazS/uc/n08MPP6y33npLxcXFys3N1XXXXaff//73MplMJ3y93+/X/v37lZycfFL7AwAAAABwOgzDUGVlpXJzc2U2H38MO6qB+4knntCMGTP0xhtvqHfv3lqxYoWuv/56paSk6Pbbbz/h6/fv36+8vLwIVAoAAAAAwHcKCwvVvn374+4T1cD93//+V2PHjtWYMWMkSR07dtTf//53ffXVVyf1+uTkZEkNb9TpdIatTgAAAAAAJMnlcikvLy+QR48nqoH7nHPO0YsvvqitW7eqe/fuWrt2rb744gtNnz79qPu73W653e7AdmVlpSQpPj5e8fHxEakZAAAAABC7PB6PJJ3UZc1RDdz333+/XC6XevbsKYvFIp/Pp8cff1zjx48/6v5Tp07VlClTGrXPnTtXCQkJ4S4XAAAAABDjampqTnpfk2EYRhhrOa6ZM2fq3nvv1R//+Ef17t1ba9as0Z133qnp06drwoQJjfb/4Qj3kaH80tJSppQDAAAAAMLO5XIpPT1dFRUVJ8yhUQ3ceXl5uv/++zVx4sRA22OPPaa33npLmzdvPuHrXS6XUlJSTuqNAgAAAABwukLJoVG9D3dNTU2jZdQtFov8fn+UKgIAAAAAoGlE9RruSy+9VI8//rg6dOig3r17a/Xq1Zo+fbpuuOGGaJYFAAAAAMBpi+qU8srKSj3wwAOaPXu2SkpKlJubq3HjxunBBx+U3W4/4euZUg4AAAAAiKRQcmhUA/fpInADAAAAACKpxVzDDQAAAABAa0XgBgAAAAAgDKK6aFqsWL+3QnsP16h7drK6ZCRFuxwAAAAAQAQwwh0BbyzdpVv+tkpzvz4Q7VIAAAAAABFC4I4A07f/NdRi16cDAAAAAISIwB0Bpm8Td8tdDx4AAAAAECoCdwSYjyRuAAAAAEDMIHBHwJG87fczxA0AAAAAsYLAHRENiZu4DQAAAACxg8AdAVzDDQAAAACxh8AdAaxSDgAAAACxh8AdAYxwAwAAAEDsIXBHwJFVyg0SNwAAAADEDAJ3BHw3pRwAAAAAECsI3BFgCoxwR7kQAAAAAEDEELgjiEXTAAAAACB2ELgjgEXTAAAAACD2ELgjwPTtVdzkbQAAAACIHQTuCDgywu1niBsAAAAAYgaBOwLMLFMOAAAAADGHwB0BgVXKo1wHAAAAACByCNwREBjgZko5AAAAAMQMAncksEo5AAAAAMQcAncEHFml3E/gBgAAAICYQeCOgMB9uLmKGwAAAABiBoE7AsxMKQcAAACAmBPVwN2xY0eZTKZGXxMnToxmWU3uyJTy1/+7i4XTAAAAACBGRDVwL1++XEVFRYGvefPmSZKuuuqqaJbV5I5MKZekjUWu6BUCAAAAAIgYazS/eUZGRtD2tGnT1KVLF11wwQVRqig8vpe3mVYOAAAAADEiqoH7++rr6/XWW29p0qRJMn1/SPh73G633G53YNvlahgt9ng88ng8EanzVPj9/sBjr9fbrGsFAAAAABxbKHmu2QTu999/X+Xl5bruuuuOuc/UqVM1ZcqURu1z585VQkJCGKs7PdsLzToye3/Jki+0OzG69QAAAAAATk1NTc1J72symskqXqNGjZLdbteHH354zH2ONsKdl5en0tJSOZ3OSJR5Sp7+dLue+WyHJOn9W85W79zmWysAAAAA4NhcLpfS09NVUVFxwhzaLEa4d+/erfnz5+u999477n4Oh0MOh6NRu81mk81mC1d5p81stgQeW63WZl0rAAAAAODYQslzzeI+3K+99poyMzM1ZsyYaJcCAAAAAECTiHrg9vv9eu211zRhwgRZrc1iwL3JNYs5+wAAAACAiIp64J4/f7727NmjG264IdqlRMQxFmAHAAAAALQyUR9SHjlypJrJum0AAAAAADSZqI9wAwAAAADQGhG4I8wk5pQDAAAAQCwgcEcCU+YBAAAAIOYQuCOMRdMAAAAAIDYQuAEAAAAACAMCdwQwoRwAAAAAYg+BO8KYUg4AAAAAsYHADQAAAABAGBC4AQAAAAAIAwJ3hHGHMAAAAACIDQTuCPh+yCZwAwAAAEBsIHBHmMGa5QAAAAAQEwjcEcYINwAAAADEBgJ3BHArMAAAAACIPQTuCDB9L3Ezwg0AAAAAsYHAHQHm741wcw03AAAAAMQGAncEmBnhBgAAAICYQ+COgOARbgAAAABALCBwR0DwNdxEbgAAAACIBQTuCLB8b4ibuA0AAAAAsYHAHQFBU8pJ3AAAAAAQEwjcEWAOuhE3iRsAAAAAYgGBOwK4DzcAAAAAxB4CdwSwSjkAAAAAxB4CdwRwH24AAAAAiD1RD9z79u3Tz3/+c7Vt21bx8fHq27evVqxYEe2ymlTwomkkbgAAAACIBdZofvPDhw9r2LBh+tGPfqSPP/5YGRkZ2rZtm9q0aRPNsppc0DXcUawDAAAAABA5UQ3cTzzxhPLy8vTaa68F2jp16hTFisKDKeUAAAAAEHuiOqX8X//6lwYPHqyrrrpKmZmZGjBggF566aVolhQWualxgccGY9wAAAAAEBOiOsK9Y8cOzZgxQ5MmTdJvf/tbLV++XLfffrvsdrsmTJjQaH+32y232x3YdrlckiSPxyOPxxOxukN1TqfUwGOv19usawUAAAAAHFsoeS6qgdvv92vw4MH6wx/+IEkaMGCANmzYoOeff/6ogXvq1KmaMmVKo/a5c+cqISEh7PWejpx4i4pqTfpy2Vc6vJlRbgAAAABoiWpqak5636gG7pycHPXq1Suo7YwzztC777571P0nT56sSZMmBbZdLpfy8vI0cuRIOZ3OsNZ6uv66478qqq3SkCFDNKxL22iXAwAAAAA4BUdmWp+MqAbuYcOGacuWLUFtW7duVX5+/lH3dzgccjgcjdptNptsNltYamwqR1Yqt1gszb5WAAAAAMDRhZLnorpo2l133aUvv/xSf/jDH7R9+3a9/fbbevHFFzVx4sRolhUWRwI3q5QDAAAAQGyIauA+66yzNHv2bP39739Xnz599Oijj+qpp57S+PHjo1lWWBy5MRh5GwAAAABiQ1SnlEvST37yE/3kJz+Jdhlhd+RW3AZD3AAAAAAQE6I6wh1LAoE7umUAAAAAACKEwB0h5sA13ERuAAAAAIgFBO4ICVzDTd4GAAAAgJhA4I4UVikHAAAAgJhC4I4QVikHAAAAgNhC4I4QVikHAAAAgNhC4I4QRrgBAAAAILYQuCPEYm6I3H4/kRsAAAAAYgGBO0Ks5oYftZfADQAAAAAxIeTAXVtbq5qamsD27t279dRTT2nu3LlNWlhrY7U0jHB7/f4oVwIAAAAAiISQA/fYsWP15ptvSpLKy8s1dOhQ/elPf9LYsWM1Y8aMJi+wtbB+O6Xc42OEGwAAAABiQciBe9WqVTrvvPMkSbNmzVJWVpZ2796tN998U08//XSTF9haWC0NP2ofU8oBAAAAICaEHLhramqUnJwsSZo7d66uvPJKmc1mnX322dq9e3eTF9haHBnh9vqYUg4AAAAAsSDkwN21a1e9//77Kiws1H/+8x+NHDlSklRSUiKn09nkBbYWR0a4mVIOAAAAALEh5MD94IMP6p577lHHjh01dOhQFRQUSGoY7R4wYECTF9haHBnhZko5AAAAAMQGa6gv+J//+R+de+65KioqUv/+/QPtw4cP1xVXXNGkxbUmgUXTWKUcAAAAAGJCyIFbkrKzs5WdnS1Jcrlc+vTTT9WjRw/17NmzSYtrTY5MKfcypRwAAAAAYkLIU8qvvvpqPfvss5Ia7sk9ePBgXX311erXr5/efffdJi+wtdheUilJenrBtihXAgAAAACIhJAD9+LFiwO3BZs9e7YMw1B5ebmefvppPfbYY01eYGuxpbghcHu5hhsAAAAAYkLIgbuiokJpaWmSpE8++UQ//elPlZCQoDFjxmjbNkZvj+W+0Q3T7XtkJUe5EgAAAABAJIQcuPPy8rR06VJVV1frk08+CdwW7PDhw4qLi2vyAluLnJSGn02cLeQfOQAAAACgBQp50bQ777xT48ePV1JSkvLz83XhhRdKaphq3rdv36aur9WwfbtoWj2LpgEAAABATAg5cN96660aMmSICgsLdfHFF8tsbgiSnTt35hru4zgSuD0+bgsGAAAAALHglG4LNnjwYA0ePFiGYcgwDJlMJo0ZM6apa2tVbJZv78NN4AYAAACAmHBKFxS/+eab6tu3r+Lj4xUfH69+/frp//7v/5q6tlYlMMLtJXADAAAAQCwIeYR7+vTpeuCBB3Tbbbdp2LBhkqQvvvhCN998s0pLS3XXXXc1eZGtwZHAvb+iTj6/IYvZFOWKAAAAAADhFPII9zPPPKMZM2boiSee0GWXXabLLrtMTz75pP7617/q6aefDulYDz/8sEwmU9BXz549Qy2pRfAb3y2Wtm5vefQKAQAAAABERMgj3EVFRTrnnHMatZ9zzjkqKioKuYDevXtr/vz53xVkPaXLypu9dqnx0S4BAAAAABBBIY9wd+3aVf/4xz8atb/zzjvq1q1byAVYrVZlZ2cHvtLT00M+RkuQmmCX/dtp5bX1vihXAwAAAAAIt5CHk6dMmaJrrrlGixcvDlzDvWTJEi1YsOCoQfxEtm3bptzcXMXFxamgoEBTp05Vhw4djrqv2+2W2+0ObLtcLkmSx+ORx+MJ+XtHWse2CdpaUqX3V+/VWfkp0S4HAAAAABCiULKnyTC+d3HxSVq5cqX+/Oc/a9OmTZKkM844Q3fffbcGDBgQ0nE+/vhjVVVVqUePHioqKtKUKVO0b98+bdiwQcnJyY32f/jhhzVlypRG7W+//bYSEhJCfRsRd8fS785v/KXAG8VKAAAAAACnoqamRtdee60qKirkdDqPu+8pBe5wKS8vV35+vqZPn64bb7yx0fNHG+HOy8tTaWnpCd9oc3DOEwt1sKpekrT1kYtlMrFSOQAAAAC0JC6XS+np6ScVuE9qSvmRqdsn43SCb2pqqrp3767t27cf9XmHwyGHw9Go3WazyWaznfL3jZTZE4fp3Cc+kyRtPFCjM/NSo1sQAAAAACAkoWTPkwrcqampJxyNNQxDJpNJPt+pLwhWVVWlb775Rr/4xS9O+RjNWW7KdyuV3zFztRbd+6MoVgMAAAAACKeTCtyfffZZWL75Pffco0svvVT5+fnav3+/HnroIVksFo0bNy4s3y/azObvTlq4apv/Im8AAAAAgFN3UoH7ggsuCMs337t3r8aNG6dDhw4pIyND5557rr788ktlZGSE5fs1J83mwnkAAAAAQFiEfFuwpjRz5sxofvuoGts/N9olAAAAAADCyBztAmLNded0lCQlxUX1XAcAAAAAIMwI3BFmtzb8yD0+JpUDAAAAQGtG4I6wardXkvTi4h1RrgQAAAAAEE4hB+6HHnpIu3fvDkctMeFfa/ZHuwQAAAAAQASEHLg/+OADdenSRcOHD9fbb78tt9sdjrpare/fztzvZ1o5AAAAALRWIQfuNWvWaPny5erdu7fuuOMOZWdn65ZbbtHy5cvDUV+r0yvXGXhcwb24AQAAAKDVOqVruAcMGKCnn35a+/fv1yuvvKK9e/dq2LBh6tevn/7yl7+ooqKiqetsNf74P/0Djwc8Ok9enz+K1QAAAAAAwuW0Fk0zDEMej0f19fUyDENt2rTRs88+q7y8PL3zzjtNVWOrkpeWELS9ubgySpUAAAAAAMLplAL3ypUrddtttyknJ0d33XWXBgwYoE2bNmnRokXatm2bHn/8cd1+++1NXWurtKawXHUeX7TLAAAAAAA0sZADd9++fXX22Wdr586deuWVV1RYWKhp06apa9eugX3GjRungwcPNmmhrcn53TMCj3///gbdO2tdFKsBAAAAAIRDyIH76quv1q5du/Tvf/9bl19+uSwWS6N90tPT5fdzbfKxpCXYgrY/XMutwgAAAACgtQk5cD/wwANq166dpIZruA2DW1uF6r7RPaNdAgAAAAAgzE7pGu5XXnlFffr0UVxcnOLi4tSnTx+9/PLLTV1bq5WTEh/tEgAAAAAAYWYN9QUPPvigpk+frl//+tcqKCiQJC1dulR33XWX9uzZo0ceeaTJi2yNxvTL0b/XFUW7DAAAAABAmIQcuGfMmKGXXnpJ48aNC7Rddtll6tevn379618TuE+S389UfAAAAABozUKeUu7xeDR48OBG7YMGDZLX622SomLBbRd1PfFOAAAAAIAWK+TA/Ytf/EIzZsxo1P7iiy9q/PjxTVJULOidmxLtEgAAAAAAYRTylHKpYdG0uXPn6uyzz5YkLVu2THv27NEvf/lLTZo0KbDf9OnTm6bKGOD1+WW1nNIadgAAAACAZijkwL1hwwYNHDhQkvTNN99Iarjvdnp6ujZs2BDYz2QyNVGJrdctF3bRjIUNP8OpH2/WAz/pFeWKAAAAAABNxWS04Btpu1wupaSkqKKiQk6nM9rlhMwwDHWaPCewvWvamChWAwAAAAA4kVBy6GnNYd67d6/27t17OoeIacwCAAAAAIDWK+TA7ff79cgjjyglJUX5+fnKz89XamqqHn30Ufn9/nDU2KrdcmGXwOMWPNkAAAAAAPADIV/D/bvf/U6vvPKKpk2bpmHDhkmSvvjiCz388MOqq6vT448/3uRFtmZ9vrda+d7DtcpLS4hiNQAAAACAphJy4H7jjTf08ssv67LLLgu09evXT+3atdOtt95K4A5RRa0n8HjSP9bonzefE8VqAAAAAABNJeQp5WVlZerZs2ej9p49e6qsrKxJiool53ZNDzxevuuwqtzeKFYDAAAAAGgqIQfu/v3769lnn23U/uyzz6p///5NUlQs6dA2eAr5Mwu2RakSAAAAAEBTCnlK+ZNPPqkxY8Zo/vz5KigokCQtXbpUhYWFmjNnzglefWzTpk3T5MmTdccdd+ipp5465eO0dIWHa6JdAgAAAACgCYQ8wn3BBRdo69atuuKKK1ReXq7y8nJdeeWV2rJli84777xTKmL58uV64YUX1K9fv1N6fWtiNZ/WndoAAAAAAM1ESOnO4/Fo+PDhqq6u1uOPP653331X7777rh577DHl5uaeUgFVVVUaP368XnrpJbVp0+aUjtHSvX3T0MDjf63dH8VKAAAAAABNJaQp5TabTevWrWvSAiZOnKgxY8ZoxIgReuyxx467r9vtltvtDmy7XC5JDScCPB7PsV7W7GUl2YK2SyqqlWi3ym5ltBsAAAAAmpNQsmfI13D//Oc/D9yH+3TNnDlTq1at0vLly09q/6lTp2rKlCmN2ufOnauEhJZ+/+rvumLI1IXKSzR0Tz9fFOsBAAAAAPxQTc3Jr7sVcuD2er169dVXNX/+fA0aNEiJiYlBz0+fPv2kjlNYWKg77rhD8+bNU1xc3Em9ZvLkyZo0aVJg2+VyKS8vTyNHjpTT6Tz5N9EM3bF0btB2YbVJo0ePlslkilJFAAAAAIAfOjLT+mSEHLg3bNiggQMHSpK2bt0a6ssDVq5cqZKSksCxJMnn82nx4sV69tln5Xa7ZbFYgl7jcDjkcDgaHctms8lmszVqb+ncfpOS41rf+wIAAACAliqU7Bly4P7ss89CfclRDR8+XOvXrw9qu/7669WzZ0/dd999jcJ2a/ezs/I0c3lhUFtFrYfADQAAAAAtVMirct1www2qrKxs1F5dXa0bbrjhpI+TnJysPn36BH0lJiaqbdu26tOnT6hltXgPX9ZbE3/UJait2s013AAAAADQUoUcuN944w3V1tY2aq+trdWbb77ZJEXFojibRbde2DWo7VC1+xh7AwAAAACau5OeUu5yuWQYhgzDUGVlZdBCZz6fT3PmzFFmZuZpFbNw4cLTen1Ll+iwatG9F+qCPy6UJD35yRa9PzE9ukUBAAAAAE7JSQfu1NRUmUwmmUwmde/evdHzJpPpqLfsQmjy23636vuawnIVVdQqJyU+ihUBAAAAAE7FSQfuzz77TIZh6KKLLtK7776rtLS0wHN2u135+fnKzc0NS5Gx5rHL++j372+QJP3tyz26Z1SPKFcEAAAAAAjVSQfuCy64QJK0c+dO5eXlyWwO+fJvnKT2bb4b0TZzG24AAAAAaJFCvi1Yfn6+ysvL9dVXX6mkpER+vz/o+V/+8pdNVhzEbcEAAAAAoIUKOXB/+OGHGj9+vKqqquR0OmUyfTcEazKZCNxN4IwcZ+Dx43M26eqz8pQST/AGAAAAgJYk5Hnhd999t2644QZVVVWpvLxchw8fDnyVlZWFo8aYk+WMC9r+YM2+KFUCAAAAADhVIQfuffv26fbbb1dCQkI46sG3Eu2WwOMN+yqiWAkAAAAA4FSEHLhHjRqlFStWhKMWfI/b+9218e+t2qeKWk8UqwEAAAAAhCrka7jHjBmje++9Vxs3blTfvn1lswVfW3zZZZc1WXGxzBlvU1l1vSTJ6zc07sUvNeeO86JcFQAAAADgZJkMwzBCecHxbgdmMpnk8/lOu6iT5XK5lJKSooqKCjmdzhO/oAVZteewrvzrf4Padk0bE6VqAAAAAABSaDk05Cnlfr//mF+RDNut3cAObbRhyqigNq7lBgAAAICWI+TA/X11dXVNVQeOIslh1aJ7Lwxs/+SZL6JXDAAAAAAgJCEHbp/Pp0cffVTt2rVTUlKSduzYIUl64IEH9MorrzR5gbEuv21itEsAAAAAAJyCkAP3448/rtdff11PPvmk7HZ7oL1Pnz56+eWXm7Q4NGiXGh94vPdwTRQrAQAAAACcrJAD95tvvqkXX3xR48ePl8Xy3b2i+/fvr82bNzdpcWjw9LgzA4/X7+U6bgAAAABoCUIO3Pv27VPXrl0btfv9fnk83Cs6HM7I+W7lu1v+tkolLq6dBwAAAIDmLuTA3atXL33++eeN2mfNmqUBAwY0SVEIlmC36oZhnQLbv529PorVAAAAAABOhjXUFzz44IOaMGGC9u3bJ7/fr/fee09btmzRm2++qY8++igcNULSyN5ZenXJTknSjtLqKFcDAAAAADiRkEe4x44dqw8//FDz589XYmKiHnzwQW3atEkffvihLr744nDUCEk5KXGBx1azKYqVAAAAAABORsgj3JJ03nnnad68eU1dC44jM/m7wL31QJXcXp8cVstxXgEAAAAAiKaQR7gRHfH24HDNauUAAAAA0LwRuFuQfu1TAo9X7j4cxUoAAAAAACdC4G5Baut9gcdTP+ae5wAAAADQnBG4W5DOGYlB2/Vef5QqAQAAAACcyGkHbp/PpzVr1ujwYaY4h9ujl/cJ2t5U5IpSJQAAAACAEwk5cN9555165ZVXJDWE7QsuuEADBw5UXl6eFi5cGNKxZsyYoX79+snpdMrpdKqgoEAff/xxqCXFjMzkOG1+9JLA9tjnlmj6vK1RrAgAAAAAcCwhB+5Zs2apf//+kqQPP/xQO3fu1ObNm3XXXXfpd7/7XUjHat++vaZNm6aVK1dqxYoVuuiiizR27Fh9/fXXoZYVM+JsFqUn2QPbTy/YpooaTxQrAgAAAAAcTciBu7S0VNnZ2ZKkOXPm6KqrrlL37t11ww03aP369SEd69JLL9WPf/xjdevWTd27d9fjjz+upKQkffnll6GWFVP6tEsJ2v5o/f4oVQIAAAAAOJaQA3dWVpY2btwon8+nTz75RBdffLEkqaamRhaL5QSvPjafz6eZM2equrpaBQUFp3ycWGC3BHfbvI0HolQJAAAAAOBYrKG+4Prrr9fVV1+tnJwcmUwmjRgxQpK0bNky9ezZM+QC1q9fr4KCAtXV1SkpKUmzZ89Wr169jrqv2+2W2+0ObLtcDYuGeTweeTyxNK3aCNpKT7TH2PsHAAAAgOgIJXuFHLgffvhh9enTR4WFhbrqqqvkcDgkSRaLRffff3+oh1OPHj20Zs0aVVRUaNasWZowYYIWLVp01NA9depUTZkypVH73LlzlZCQEPL3bqlKDpj1/ckJX+8o1Jw5u6NXEAAAAADEiJqampPe12QYhnHi3Y6vvLxcqampp3sYSdKIESPUpUsXvfDCC42eO9oId15enkpLS+V0Opvk+7cEd7yzVnM2fDeNvGd2sj6cyDR8AAAAAAg3l8ul9PR0VVRUnDCHhjzC/cQTT6hjx4665pprJElXX3213n33XeXk5GjOnDnq16/fqVX9Lb/fHxSqv8/hcARG1L/PZrPJZrOd1vdtSVIS7EHbhWU1slisMptNUaoIAAAAAGJDKNkz5EXTnn/+eeXl5UmS5s2bp3nz5unjjz/WJZdconvuuSekY02ePFmLFy/Wrl27tH79ek2ePFkLFy7U+PHjQy0rptx1cXf1ynHqoUsbpt1X1/u0eNvBKFcFAAAAAPi+kEe4i4uLA4H7o48+0tVXX62RI0eqY8eOGjp0aEjHKikp0S9/+UsVFRUpJSVF/fr103/+85/Ayuc4uszkOM254zxJ0oyF36ik0q3rXluuz3/zI+Wlxc617AAAAADQnIU8wt2mTRsVFhZKkj755JPAKuWGYcjn84V0rFdeeUW7du2S2+1WSUmJ5s+fT9gO0bVDOwQe3//euihWAgAAAAD4vpAD95VXXqlrr71WF198sQ4dOqTRo0dLklavXq2uXbs2eYE4vpG9sgOPl2w/pCZYAw8AAAAA0ARCnlL+5z//WR07dlRhYaGefPJJJSUlSZKKiop06623NnmBOL7c1LigbVetVykJsbOAHAAAAAA0VyEHbpvNdtTF0e66664mKQihSYkPDtcHKusI3AAAAADQDIQcuCXpm2++0VNPPaVNmzZJknr16qU777xTnTt3btLicGImU/CtwIor6tQ9KzlK1QAAAAAAjgj5Gu7//Oc/6tWrl7766iv169dP/fr107Jly9SrVy/NmzcvHDXiBEb1zgo8Xrz1oLYdqORabgAAAACIMpMRYjIbMGCARo0apWnTpgW133///Zo7d65WrVrVpAUej8vlUkpKiioqKuR0OiP2fZubiX9bpX+vLwpqu2tEd90xoluUKgIAAACA1imUHBryCPemTZt04403Nmq/4YYbtHHjxlAPhyZgtZgatf15/tYoVAIAAAAAOCLkwJ2RkaE1a9Y0al+zZo0yMzOboiaEyGo+ejfW1HsjXAkAAAAA4IiQF0276aab9Ktf/Uo7duzQOeecI0lasmSJnnjiCU2aNKnJC8SJjemXrXdX7W3UvnhrqS7pk32UVwAAAAAAwi3kwP3AAw8oOTlZf/rTnzR58mRJUm5urh5++GHdfvvtTV4gTuxHPTL1z5sLdNXzS4PabUeZag4AAAAAiIyQArfX69Xbb7+ta6+9VnfddZcqKyslScnJ3IYqmkwmk87qmNao3WImcAMAAABAtIR0DbfVatXNN9+suro6SQ1Bm7DdfP15HgunAQAAAEC0hLxo2pAhQ7R69epw1ILTdG7X9KDttXsrWDgNAAAAAKIk5Gu4b731Vt19993au3evBg0apMTExKDn+/Xr12TFITT/76r+OnvqgqC2EpdbHdND7mYAAAAAwGkKOYn97Gc/k6SgBdJMJpMMw5DJZJLP52u66hCS7JQ4XTGgnWav3hdoO1TtVsf0xOO8CgAAAAAQDiEH7p07d4ajDjSRc7q0DQrc9V4jitUAAAAAQOwyGYbRYhOZy+VSSkqKKioq5HQ6o11Os+DzG+ry2zmB7d65Tr0/cZhslpAv1wcAAAAA/EAoOTTkFDZ16lS9+uqrjdpfffVVPfHEE6EeDk3MYjZp3l3nB7a/3u/SEx9vjmJFAAAAABCbQg7cL7zwgnr27NmovXfv3nr++eebpCicnm5ZyRryvftyv/wFlwEAAAAAQKSFHLiLi4uVk5PTqD0jI0NFRUVNUhRO37aSyqDtsur6KFUCAAAAALEp5MCdl5enJUuWNGpfsmSJcnNzm6QonL7DNZ6g7dmr98nvb7GX6wMAAABAixPyKuU33XST7rzzTnk8Hl100UWSpAULFug3v/mN7r777iYvEE3j0Y82am1huZ4eNyDapQAAAABATAg5cN977706dOiQbr31VtXXN0xTjouL03333afJkyc3eYE4NZf0ztYnXxcHtf1r7X4CNwAAAABEyCnfFqyqqkqbNm1SfHy8unXrJofD0dS1nRC3BTu2Oo9PG4tcuumNFTr0veu3d00bE8WqAAAAAKBlC+ttwY5ISkrSWWedpT59+kQlbOP44mwWDezQRq9fPyTapQAAAABATDrlwI2WoU87Rv4BAAAAIBqiGrinTp2qs846S8nJycrMzNTll1+uLVu2RLOkVsdkMumVCYMD2x+t2x/FagAAAAAgdkQ1cC9atEgTJ07Ul19+qXnz5snj8WjkyJGqrq6OZlmtTt/2KYHHt729Whv2VUSxGgAAAACIDae8aFo4HDx4UJmZmVq0aJHOP//8E+7Pomknp7bepzMe/CSojcXTAAAAACB0oeTQkG8LFk4VFQ0jr2lpaUd93u12y+12B7ZdLpckyePxyOPxhL/AFspqatz2k6c/19s3nqV4uyXyBQEAAABACxVK9mw2gdvv9+vOO+/UsGHD1KdPn6PuM3XqVE2ZMqVR+9y5c5WQkBDuElu44K7esN+lP82cq4HpzWaCAwAAAAA0ezU1NSe9b7OZUn7LLbfo448/1hdffKH27dsfdZ+jjXDn5eWptLSUKeUn0O2BuY3arivooN/9uGcUqgEAAACAlsnlcik9Pb3lTCm/7bbb9NFHH2nx4sXHDNuS5HA4jnrPb5vNJpvNFs4SW7xB+W20cvfhoLbXl+7RfaN7Ma0cAAAAAE5SKNkzqquUG4ah2267TbNnz9ann36qTp06RbOcVu3VCWfpunM6NmqvdHPtOwAAAACEQ1QD98SJE/XWW2/p7bffVnJysoqLi1VcXKza2tpoltUqpSTYNOKMrEbtbo8/CtUAAAAAQOsX1cA9Y8YMVVRU6MILL1ROTk7g65133olmWa1Wvc/XqG3BpgNRqAQAAAAAWr+oXsPdTNZrixn5bRMbtT384UZdN4yp/AAAAADQ1KI6wo3I6pKRpDdvGKILumcEtXPiAwAAAACaHoE7xpzfPUMTzskParv+9eVRqgYAAAAAWi8Cdwz6UY9M3XfJd/ffXrjlYBSrAQAAAIDWicAdg0wmk265sEtQW029N0rVAAAAAEDrROCOYVsfGx14/Pi/N2nv4ZooVgMAAAAArQuBO4bZrWa1S42XJP1t2R6d+8RnUa4IAAAAAFoPAneM21deG7Rd5WZqOQAAAAA0BQI3gnAtNwAAAAA0DQI3guwvr4t2CQAAAADQKhC4Y9wZOc6g7cufW6LXluyMUjUAAAAA0HoQuGPc8z8fqCsHtAtqm/LhxihVAwAAAACtB4E7xuW3TdT0a85s1O7x+SNfDAAAAAC0IgRuHNWTn2yOdgkAAAAA0KIRuCFJGjckL2j7pc93am1heXSKAQAAAIBWgMANSdIfruirWTcXBLWNfW5JlKoBAAAAgJaPwA1Jkslk0uCOaZo/6YJolwIAAAAArQKBG0G6ZiYFbS/YdCBKlQAAAABAy0bgxnHNXF4Y7RIAAAAAoEUicKOR3485I/B43sYDen3JzihWAwAAAAAtE4EbjfzveZ31k345ge2HP9wYxWoAAAAAoGUicOOoqt3eaJcAAAAAAC0agRtH5fEZQduzVu6NUiUAAAAA0DIRuHFUV5+VF7R9zz/Xav5GViwHAAAAgJNF4MZRXdovR69df1ZQ2/++uUJFFbVRqggAAAAAWhYCN47KZDLpRz0ydeXAdkHtY59dEqWKAAAAAKBlIXDjuBLt1qDtkkq31hSWR6cYAAAAAGhBohq4Fy9erEsvvVS5ubkymUx6//33o1kOjiLBYWnUdvlzS2QYxlH2BgAAAAAcEdXAXV1drf79++u5556LZhk4jv89t/NR23/24pcRrgQAAAAAWhbriXcJn9GjR2v06NHRLAEnkJHs0I4//Fhms0lz1hfp1r+tkiQt21mmd1fu1U8HtY9yhQAAAADQPEU1cIfK7XbL7XYHtl0ulyTJ4/HI4/FEq6yY4PNJF/dM18VnZGrephJJ0t3/XKuMJKsKOreNcnUAAAAAEBmhZM8WFbinTp2qKVOmNGqfO3euEhISolBR7LkgUZr3vX82k95eoQcG+qJYEQAAAABETk1NzUnvazKayepXJpNJs2fP1uWXX37MfY42wp2Xl6fS0lI5nc4IVAlJumfWen2wtiiwvWXKxTKbTVGsCAAAAAAiw+VyKT09XRUVFSfMoS1qhNvhcMjhcDRqt9lsstlsUagoNv1l3EAN7bxHv529XpJ0xz/W66mfnak4W+MVzQEAAACgNQkle3IfbpySEWdkBh5/8nWx/jx/axSrAQAAAIDmJ6qBu6qqSmvWrNGaNWskSTt37tSaNWu0Z8+eaJaFk5DpjNN53dID268v2RW9YgAAAACgGYpq4F6xYoUGDBigAQMGSJImTZqkAQMG6MEHH4xmWThJEwo6Bh67vf7oFQIAAAAAzVCzWTTtVLhcLqWkpJzUxepoel6fX9e+vExf7SwLtN0wrJMevLRXFKsCAAAAgPAJJYdyDTdOmdVi1ju/Ojuo7dUlO1Xt9kapIgAAAABoPgjcOC0mk0kJ9uDVyf/vy92SpBY8eQIAAAAAThtTynHaCstqdN6Tnx31ub+OH6gf982JcEUAAAAAEB5MKUdE5aUl6Gdn5R31uVv/tkqSVOKq09SPN2nPoZpIlgYAAAAAUUPgRpOY9tN+en/isKM+948VhZr0j7V6YdEOjXvpywhXBgAAAADRQeBGkzkzL1WrHri4UftvZq3TF9tLJUn7ymsjXRYAAAAARAWBG00qLdGuXdPGaNMjl0S7FAAAAACIKgI3wiLebtFbNw496nM/nfFf3fD6ck2ds0ll1fURrgwAAAAAIsMa7QLQep3bLV192jm1YZ8rqH3l7sOSpE83l2jd3gr9/Qf38gYAAACA1oARboTVR78+T+/eUnDM55fuOKSXFu/Q1I83ye31RbAyAAAAAAgv7sONiNh7uEaGoWPer1uSxg/toMcu7yOTyRTBygAAAADg5IWSQwnciKg564sC9+Y+mvZt4jVj/CAdcNUp3m6Rw2pWepJDHdMTI1glAAAAABwdgRvN2rYDlXpq/jYt3nZQlXXek3rN11NGKdHBkgMAAAAAoovAjRbj080HdMPrK056/3ibRbNuKVDv3BR5fX5JktXCUgQAAAAAIoPAjRalotajA6463fq3VdpeUhXSa/PS4tU9M1kLNpeoTYJN8yddoLZJjjBVCgAAACDWEbjRYtXUe/X72Ru0cs9h7T5Uc8rH+f8u6KzclHhlp8RpVO/sJqwQAAAAQCwjcKNVWL3nsJ5f9I3GD83Xi4t36LIzc/WbWetCPk6vHKf2Hq6Rq86rHlnJ2nKgUskOq6Zfc6bOzEvVyt1l6pqZrK6ZSWF4FwAAAABaEwI3Wq31eys0a2Wh7FazXvp8Z5Meu0tGooZ1TdfYM9tpQF6qquu9clgtslu5RhwAAABAAwI3Wj2Pz68Fmw5oSKe2apNg0/aSKjnjbSosq9GrS3YqPcmhTzeXaO/h2ojU07ddijqlJ+r/u6Cz6jx+pSXale2Mk9Vi0j9X7FWP7CR1yUiSM86mI7cZ537jAAAAQMtD4Aa+tWrPYZW46nRGjlNbiiv1xtJdWrL9ULTLCvLgT3rpkY82aky/HP17XZEk6YVfDFLbRLvsVrPW7q3Qv9bsU05KvHaX1ejGcztpZK8sbS+p0pc7Dml03xy1S42XJFW7vTKbTPpsS4kykh06q2NaNN8aAAAA0OoQuIHjKKuu1/+bu0U/HdheVrNJVW6v3vpyt8qq6zWmX44e/ODraJcYFu1S49U5I1GD8tvo6/0udUhL0CtfNEzLv/HcTiqqqNWki7urfZsEvbl0l3YcrFbvdimqq/fp8gHtVFPvVXKcTWaT9N6qfUqOs2pNYbl+dlYHZafEKTmu4T7ph6rrteNglc7tmq4qt1df73dpaKc0mUwmGYZxwpH9Oo9PrjqPMpPjTul9+vyGvH6/HFbLKb0eAAAAOB4CN3Aa/H5DXr8RuHa72u3Vu6v2akBeG7nqPLKYTbrpjRW6fEA7rd9XoYEd2mjJ9lIlOizqnJGkWSv3RvkdtFx2i1n1395f3W41q97b8LhbZpISHFZNvLCL9pfXatehGmWnxCk/LUFf73cpzmbWBd0z9deF2/Xp5hK5vX79+qKuOuCqU7fMZKXE21Rd79XoPjkym6Tkb6f2m0zSku2l6tc+VQ6rWUkOq+ZvKtGWYpeuHZqvsup6lbjqZLeaZTabtGDTAY04I0v926d++/qGkweFZTVye/1KTbCptt6n9m0aZhx8/+TCyZxscHt9WrnrsM7qlCYb95cHAABolgjcQDOw51CN7Fazqtxedc1M0uo9h7Vy92H9/Ox8xdksMgxDWw9UyePzy+316+a3Vur8bhnKTnHouc++iXb5CKOclDgdcNXJam44wdC/fYrW7q04pWOlJznUOSNRX+0skyR1zUwK3M9+eM9Mldd6VFxRp33lDesZ3Pajrnp31V65aj3qmpmkBLtVdV6feuU4lWC36LMtBzW2f67Gn52vFxZ/I6vZpAS7VZcPaKcVu8o086tC+Y2GE1LPXjtQzjiryqrrZbWYtWR7qQ7X1OunA9tr16FquWq9ctV6lJpgU73XL6vFrK6ZSTrgqlNuarw+21yi3YdqlJHskNVsUk5qnDKSHcpNjVeN26eUeJsOVbs1Z32RzuuWoSSHVZ9vK1WczawrBrQLmjVR5/HpgKtOmckNP9v2beJl/d5JC5/f0KyVhdp6oEr3juoht9evlHibKmo8SnRYVOyqk91iVqbz+DMrDMPQlgOVSo23KzslLqj9hydUfH5DtR6fkhxW+f2GzOYTr9vg//Y1iQ7ryf0DOEX7ymuVmxLHWhIAAJwCAjcQQypqPDJkyDCk3WU1Wr6zTD8/O1/xdovcXp92llarS0aSDlXVq7LOI0PSyD8vliT95Wdnqm+7FH2936XSKreWbC/V/E0lkr4bYc5Li1dh2XeLz1nMJvn8hoZ1bdvoevjvj0oDLVGczaw6z8n/G05yWNU9K0mr9pSf9ve2mk3y+k/+T3KvHKc2FrkC28kOq87IcWpzsUvt2iToygHt9MnXxVq5+7AkaWinNHXLStLsVftUXe+TJHXOSJTXZ+iP/9NPHp8hj8+vDfsqtHDrQZVWudUjK1m7DlXL6ze042B14ORJdkqcrjiznSpqPeqWlaQ6j1/dMpO0qbhSPx3YTvvL6xRnM+vTzSXqleOUq84jV61Xxa46WcwmtW8Tr+0lVWrfJl5Vbp86ZyQq3tZwGciKXWXqnZuiIZ3S9MmGYp3fPUM7S6s1b2OxMpPj1Kddij7eUKTbh3dTncen8hqPtpdUKd5uUYe0BCU5rMpIdmhjkUvt28TLGWdTWXW99pTVaEjHNK3ac1jzNh1QSrxNF/XMVKLdqup6r7KS4xRvt8hhNau63qdDVW7tKatRmwS7MpMdWre3QnE2izKSHeqRndyoP9xen2xmc+DESkWtR5LkjLNq16EadWybEDjBUVPvVb3Xr9QEu478b5hhSGazSaVVbiXHWbV+b4UGdGgjy7fHMwxDpVX1ykh2HPffxQ9P/tR5fDIMKd5++pfZFJY1nCBzWM2crAEQ0wjcACLCMAwdqq5X20T7cf/ny+c3tG5vuVIT7EpLtMthNeuAq05ZzjjZLWYt2nZQDotZZ3VKk9lkUlFFQ8C3W8xKSbDJMKQ4m0WLth7U/y3dpaGd2irT6dC2A1Xq086pX/99tS7ulaXbftRN0+dtVWWdR78f00t/W7ZbXTKStLHIpQEdUuWq9eiFRTtU6fbKbjHrp4PaaVNRpWrqveqdm6LZq/c1qt1uMatdm3hlO+M0sneW/vvNIc3beOCo73PEGZmBExYAECtS4m06My9Vi7YebPRcepJddR6/qtzeb7cdKq1yB53MPatjGw3plKa3l+3R4RqPsp1xqvP61CMrWWaTSbsOVattkl2FZbUa0ilNCXaLPlizX5KUYLeopt6njm0TlJHs0KGqesXZLGrXJl4JdotW7TmsNgl2VdV5lZZo165D1Sqtqtf1wzqqotajb0qqtLusRvVev2rqfbqoZ6YKy2r0zcEq9cx2KtFhUdtEhz75ulhSw4nloZ3SVF7jUVl1ve4f3VNl1fXaX1GrHGec1u2t0OCOaaqp92rZzrLA34sx/XJ05YB2irNZtHBLiWo9De9vY5FLqQl2rd9boasGt1dNfcNJpB0Hq1TsqtMtF3SRq86j5xftUEGXtuqd69Su0mqlJNglw9DhGo8WbDoQuEPKiF5ZuqB7ukwmk0pcbrm9PqUnObS/vFZ7D9eqV65T8zYekMVs0uVnttO6veXy+A0546zacbBaB1x1MplMGtopTZnJDvVtn6KtByolSf3bp+rz7aVye/zy+v1aseuwrhjQTmmJdh2uafi5d0hLkNvj19f7KxRnt6hrZpKS7FbtKatRuzbx2nGwWh3TE2Q1m+XzG9r/7eyrtCS7DL+UHGfVlgOVSrRb5bCZ5bCaZbOYZbWYtKu0RllOh9burVC71HjlpcXLbDJp24Eq2a0m1dT7tPtQjdom2nV257aq8zacbPL6DVnMJllMJlW6PcpIcsjt9eubg1XKSHIoI9khr9+QzWLW5mKXfH5DaYl2uWq9apNok9dnyG8Y2rDPpby0eGUkO9Q20RE4uWUxm1Tv9au8pl51Hr/qfX7F2czy+gzlt01QrceneFvDv9Mj/14tZpOsZpOsFnPQDCiPzy9XrUdtk747sVZeUy9nnC2wj9vrk91iVk29Tz7DkN9vBJ4/EutOdELsZC5zq3J7ZbOYgtbEqan3ymIObjvRDC7DMFRT75PXZyjRYZHXbwTe+7H2r/X4lGAP70yv09XiAvdzzz2nP/7xjyouLlb//v31zDPPaMiQISd8HYEbQHPn9TX88T0yemcYUp3Xpx0Hq9WnXcpR/+hV1HjkjLc2/E+CyaSKWo8SHBZtKqpUtjNOqQk2xX17vPV7K5SV4lBGkkPlNQ3Tt79/vMKyGvn8hrK/nca+dm+Fqt1ends1XR+u268f9ciUx9cwYvnXz75Rtduruy7urn+vK9Lr/92lfeW1unNEN+WkxGlLcZX+vws6671V++T2NvxP4a5D1erfPlVWs0nDz8jS/vJa/e+bKyRJv/vxGRrQIVWPfLRR634wZX5M3xyVVNapd26K2iba9fm2UnVKT1Th4Rrlt01QvM2q99fsk9lkUl5avPLTEvT+mv1yWM26sEeG/vP10U96HM/k0T31/+ZukcdnqGd2srpnJeuL7aUqq65XnM0sm9msym9DwfFc1DNTyXFW7Ttcq52l1TpUXR9yLQAAtFRdMhJVWFYbWHenqf24b7b+On5QWI7dVFpU4H7nnXf0y1/+Us8//7yGDh2qp556Sv/85z+1ZcsWZWZmHve1BG4ACJ+TPVMeipp6r2rrfUFn75uTyjqP4m2WwKiD3zCOeRb+CFedJ3Ct+9H4/YZcdR6lJtiD2rceqNSW4kpd1LPhb12iw6o6jy8wXbesul7FFXXqleuUYRjfrsBvBE62fF+dx6c4m0UeX8Mo3RHxNos2FblUVlOvc7q0lddnqM7jkzO+YdSmYaQxIXCM2av3qVeOU6VVbvXMceq/20vVLStZGcmOwM8jPcmhWo9PByvdym/b8Fqbxax/rtgbmG6cnRKnDmkJKquu19JvDsnnN9SnXYoynQ59tHa/tpVUyWwyafG2g/rf8zrrwu4Zqqj16MN1+9UnN0WGpMPV9XJYzVq3r0I3DOskSVq1+7Aykh3qn5eqwrIaSdKzn23XhIKOWrqjVAcr3RrWNV12i1kVtR6t3Vuug5VupSbYNaRTmjzf/s/h3sO1MkkqrqiTTA0nUeKsFtV6fPrnyr0yDEOuWo/W7q3Q2Z3TVOX2asM+l8wm6ZI+2erbLlWfbj6g/eV1irdbtL2kSkM7pcluNevzbaWSpNyUOF347WjpkbYkhzUw0tvwczPJ4zv2/4YduYTn+4Z0Sgus2XAsWU6Hquq8csbbdPjbUbdoON5lRs44q1x1DT+LI6OBP7yk4oc/LwCxIc5m1sJ7fhS0Vkpz06IC99ChQ3XWWWfp2WeflST5/X7l5eXp17/+te6///7jvpbADQAAELoj01KbYuppNB2pz+vzB06OeX1+ef2GzKaGabs/nO5aWFaj9CSHTKaGy4Y8fr92llarZ3bD/0vuKq1WXlqCLGaTDlW5ZTaZFG+3fHvyxiazySSbxSybxaRiV52ykuNkNpu051CNEhwNJ8T8hqF4m0XJcbaGbb+hvYdrlZrYsA5CepJDHp9fVXVeHaqulzPeqrp6v1ISbCqqqJXVbFbdtwsodkpPlN9vqMhVJ5MaLtNqk9gwTf6jdft1QfcMHa7xqGdOspxxNrm9Ph2oaFgLYHdZjQ5WugNT/jfsq9CP++aoZ06ySlx1Skt0qE2CTX5DKqqoVbzNIledV2kJdsXbLSqvqZdM0n+3H9KeshoVdGmrmnqf4qxmJditWr+vQjX1Xg3umKa0BLtW7C5Tj+xktW+ToN2HqrW9pErdsxrq8vr92l1WI5vZ/O1sLCnZ0fB+bVazdpVWq7LOK6vFpJ8ObK8qt1f/WFGo+G/XThjYoU3D3UMq62S3WJQUZ1V6kl17ymqU7LDprS93a0CHVPVrn6p3VzWcAMxJidPh6nodrHKrZ3bDdPwBHdqorKZe+w7Xqkd2kuKsDZesbSqu1BUDcrWrtEZf7jikoZ3b6ut9FcpKiZOr1qNEu1UX9sjQvvJaHaquV73Xr/y2CfpqZ5kKD9eqfWq8LumTrZT4hve2YtdhHap2q7ber84ZidpY5NKW4kqdkePUqN5ZSku06z8birWxyKVB+Wkqr6nX+n0VykmJk8Nm0eYilwxJP+mXq7Jqt4or3OqWlaT3V+9TUUWdzuuWrq92lsnt9SvZYVWl26tRvbNks5j10boiXX5mrlLibdp6oEoHXHUa3LGNJOlwjUd1Hp/2Ha5VRrJDB1x1clgbTtiW13rUvk28zsh2Nvzbdjr0jxV71aedU5ef2U5+w1DXzCR5fYaW7jikLcWVOlRVr3Zt4mUYhnpkO/X1/oqGGWCHqtW3XYry0hLk9fl1sNKtQ9X1Gts/VxuLKpWaYAvc2WdCQb6q632qqfcqI8mhmy/sopyU+Aj8Bjl1LSZw19fXKyEhQbNmzdLll18eaJ8wYYLKy8v1wQcfHPf1BG4AAAAAQCSFkkOjejV6aWmpfD6fsrKygtqzsrK0efPmRvu73W653e7AtsvVsDqrx+ORx+MJb7EAAAAAgJgXSvZs3su//cDUqVM1ZcqURu1z585VQkJCFCoCAAAAAMSSmpqak943qoE7PT1dFotFBw4ErzZ74MABZWdnN9p/8uTJmjRpUmDb5XIpLy9PI0eOZEo5AAAAACDsjsy0PhlRDdx2u12DBg3SggULAtdw+/1+LViwQLfddluj/R0OhxyOxivb2mw22Wy2cJcLAAAAAIhxoWTPqE8pnzRpkiZMmKDBgwdryJAheuqpp1RdXa3rr78+2qUBAAAAAHDKoh64r7nmGh08eFAPPvigiouLdeaZZ+qTTz5ptJAaAAAAAAAtSdTvw306uC0YAAAAACCSQsmh5gjVBAAAAABATCFwAwAAAAAQBgRuAAAAAADCgMANAAAAAEAYRH2V8tNxZL23UG48DgAAAADAqTqSP09m/fEWHbgrKyslSXl5eVGuBAAAAAAQSyorK5WSknLcfVr0bcH8fr/279+v5ORkmUymaJdzTC6XS3l5eSosLOT2Zc0Y/dQy0E8tA/3U/NFHLQP91DLQT80ffdQytJR+MgxDlZWVys3Nldl8/Ku0W/QIt9lsVvv27aNdxklzOp3N+h8OGtBPLQP91DLQT80ffdQy0E8tA/3U/NFHLUNL6KcTjWwfwaJpAAAAAACEAYEbAAAAAIAwIHBHgMPh0EMPPSSHwxHtUnAc9FPLQD+1DPRT80cftQz0U8tAPzV/9FHL0Br7qUUvmgYAAAAAQHPFCDcAAAAAAGFA4AYAAAAAIAwI3AAAAAAAhAGBGwAAAACAMCBwR8Bzzz2njh07Ki4uTkOHDtVXX30V7ZJixsMPPyyTyRT01bNnz8DzdXV1mjhxotq2baukpCT99Kc/1YEDB4KOsWfPHo0ZM0YJCQnKzMzUvffeK6/XG+m30qosXrxYl156qXJzc2UymfT+++8HPW8Yhh588EHl5OQoPj5eI0aM0LZt24L2KSsr0/jx4+V0OpWamqobb7xRVVVVQfusW7dO5513nuLi4pSXl6cnn3wy3G+tVTlRP1133XWNPl+XXHJJ0D70U3hNnTpVZ511lpKTk5WZmanLL79cW7ZsCdqnqX7PLVy4UAMHDpTD4VDXrl31+uuvh/vttQon00cXXnhho8/SzTffHLQPfRReM2bMUL9+/eR0OuV0OlVQUKCPP/448Dyfo+bhRP3EZ6n5mTZtmkwmk+68885AW8x9ngyE1cyZMw273W68+uqrxtdff23cdNNNRmpqqnHgwIFolxYTHnroIaN3795GUVFR4OvgwYOB52+++WYjLy/PWLBggbFixQrj7LPPNs4555zA816v1+jTp48xYsQIY/Xq1cacOXOM9PR0Y/LkydF4O63GnDlzjN/97nfGe++9Z0gyZs+eHfT8tGnTjJSUFOP999831q5da1x22WVGp06djNra2sA+l1xyidG/f3/jyy+/ND7//HOja9euxrhx4wLPV1RUGFlZWcb48eONDRs2GH//+9+N+Ph444UXXojU22zxTtRPEyZMMC655JKgz1dZWVnQPvRTeI0aNcp47bXXjA0bNhhr1qwxfvzjHxsdOnQwqqqqAvs0xe+5HTt2GAkJCcakSZOMjRs3Gs8884xhsViMTz75JKLvtyU6mT664IILjJtuuinos1RRURF4nj4Kv3/961/Gv//9b2Pr1q3Gli1bjN/+9reGzWYzNmzYYBgGn6Pm4kT9xGepefnqq6+Mjh07Gv369TPuuOOOQHusfZ4I3GE2ZMgQY+LEiYFtn89n5ObmGlOnTo1iVbHjoYceMvr373/U58rLyw2bzWb885//DLRt2rTJkGQsXbrUMIyGwGE2m43i4uLAPjNmzDCcTqfhdrvDWnus+GGQ8/v9RnZ2tvHHP/4x0FZeXm44HA7j73//u2EYhrFx40ZDkrF8+fLAPh9//LFhMpmMffv2GYZhGH/961+NNm3aBPXTfffdZ/To0SPM76h1OlbgHjt27DFfQz9FXklJiSHJWLRokWEYTfd77je/+Y3Ru3fvoO91zTXXGKNGjQr3W2p1fthHhtEQEr7/P6M/RB9FR5s2bYyXX36Zz1Ezd6SfDIPPUnNSWVlpdOvWzZg3b15Qv8Ti54kp5WFUX1+vlStXasSIEYE2s9msESNGaOnSpVGsLLZs27ZNubm56ty5s8aPH689e/ZIklauXCmPxxPUPz179lSHDh0C/bN06VL17dtXWVlZgX1GjRoll8ulr7/+OrJvJEbs3LlTxcXFQf2SkpKioUOHBvVLamqqBg8eHNhnxIgRMpvNWrZsWWCf888/X3a7PbDPqFGjtGXLFh0+fDhC76b1W7hwoTIzM9WjRw/dcsstOnToUOA5+inyKioqJElpaWmSmu733NKlS4OOcWQf/paF7od9dMTf/vY3paenq0+fPpo8ebJqamoCz9FHkeXz+TRz5kxVV1eroKCAz1Ez9cN+OoLPUvMwceJEjRkzptHPMhY/T9ZoF9CalZaWyufzBf1jkaSsrCxt3rw5SlXFlqFDh+r1119Xjx49VFRUpClTpui8887Thg0bVFxcLLvdrtTU1KDXZGVlqbi4WJJUXFx81P478hya3pGf69F+7t/vl8zMzKDnrVar0tLSgvbp1KlTo2Mcea5NmzZhqT+WXHLJJbryyivVqVMnffPNN/rtb3+r0aNHa+nSpbJYLPRThPn9ft15550aNmyY+vTpI0lN9nvuWPu4XC7V1tYqPj4+HG+p1TlaH0nStddeq/z8fOXm5mrdunW67777tGXLFr333nuS6KNIWb9+vQoKClRXV6ekpCTNnj1bvXr10po1a/gcNSPH6ieJz1JzMXPmTK1atUrLly9v9Fws/l0icKNVGz16dOBxv379NHToUOXn5+sf//hHs/ogAi3Rz372s8Djvn37ql+/furSpYsWLlyo4cOHR7Gy2DRx4kRt2LBBX3zxRbRLwTEcq49+9atfBR737dtXOTk5Gj58uL755ht16dIl0mXGrB49emjNmjWqqKjQrFmzNGHCBC1atCjaZeEHjtVPvXr14rPUDBQWFuqOO+7QvHnzFBcXF+1ymgWmlIdRenq6LBZLo1X3Dhw4oOzs7ChVFdtSU1PVvXt3bd++XdnZ2aqvr1d5eXnQPt/vn+zs7KP235Hn0PSO/FyP97nJzs5WSUlJ0PNer1dlZWX0XRR17txZ6enp2r59uyT6KZJuu+02ffTRR/rss8/Uvn37QHtT/Z471j5Op5OTlyfpWH10NEOHDpWkoM8SfRR+drtdXbt21aBBgzR16lT1799ff/nLX/gcNTPH6qej4bMUeStXrlRJSYkGDhwoq9Uqq9WqRYsW6emnn5bValVWVlbMfZ4I3GFkt9s1aNAgLViwINDm9/u1YMGCoGtNEDlVVVX65ptvlJOTo0GDBslmswX1z5YtW7Rnz55A/xQUFGj9+vVBoWHevHlyOp2B6UtoWp06dVJ2dnZQv7hcLi1btiyoX8rLy7Vy5crAPp9++qn8fn/gj2tBQYEWL14sj8cT2GfevHnq0aMH05TDZO/evTp06JBycnIk0U+RYBiGbrvtNs2ePVuffvppo+n5TfV7rqCgIOgYR/bhb9mJnaiPjmbNmjWSFPRZoo8iz+/3y+128zlq5o7009HwWYq84cOHa/369VqzZk3ga/DgwRo/fnzgccx9nqK9altrN3PmTMPhcBivv/66sXHjRuNXv/qVkZqaGrTqHsLn7rvvNhYuXGjs3LnTWLJkiTFixAgjPT3dKCkpMQyj4bYEHTp0MD799FNjxYoVRkFBgVFQUBB4/ZHbEowcOdJYs2aN8cknnxgZGRncFuw0VVZWGqtXrzZWr15tSDKmT59urF692ti9e7dhGA23BUtNTTU++OADY926dcbYsWOPeluwAQMGGMuWLTO++OILo1u3bkG3myovLzeysrKMX/ziF8aGDRuMmTNnGgkJCdxuKgTH66fKykrjnnvuMZYuXWrs3LnTmD9/vjFw4ECjW7duRl1dXeAY9FN43XLLLUZKSoqxcOHCoNvg1NTUBPZpit9zR26/cu+99xqbNm0ynnvuuWZ7+5Xm5kR9tH37duORRx4xVqxYYezcudP44IMPjM6dOxvnn39+4Bj0Ufjdf//9xqJFi4ydO3ca69atM+6//37DZDIZc+fONQyDz1Fzcbx+4rPUfP1w9fhY+zwRuCPgmWeeMTp06GDY7XZjyJAhxpdffhntkmLGNddcY+Tk5Bh2u91o166dcc011xjbt28PPF9bW2vceuutRps2bYyEhATjiiuuMIqKioKOsWvXLmP06NFGfHy8kZ6ebtx9992Gx+OJ9FtpVT777DNDUqOvCRMmGIbRcGuwBx54wMjKyjIcDocxfPhwY8uWLUHHOHTokDFu3DgjKSnJcDqdxvXXX29UVlYG7bN27Vrj3HPPNRwOh9GuXTtj2rRpkXqLrcLx+qmmpsYYOXKkkZGRYdhsNiM/P9+46aabGp1MpJ/C62j9I8l47bXXAvs01e+5zz77zDjzzDMNu91udO7cOeh74NhO1Ed79uwxzj//fCMtLc1wOBxG165djXvvvTfo3sGGQR+F2w033GDk5+cbdrvdyMjIMIYPHx4I24bB56i5OF4/8Vlqvn4YuGPt82QyDMOI3Hg6AAAAAACxgWu4AQAAAAAIAwI3AAAAAABhQOAGAAAAACAMCNwAAAAAAIQBgRsAAAAAgDAgcAMAAAAAEAYEbgAAAAAAwoDADQAAAABAGBC4AQBoxa677jpdfvnl0S4DAICYROAGAAAAACAMCNwAALQCs2bNUt++fRUfH6+2bdtqxIgRuvfee/XGG2/ogw8+kMlkkslk0sKFCyVJhYWFuvrqq5Wamqq0tDSNHTtWu3btChzvyMj4lClTlJGRIafTqZtvvln19fXReYMAALRA1mgXAAAATk9RUZHGjRunJ598UldccYUqKyv1+eef65e//KX27Nkjl8ul1157TZKUlpYmj8ejUaNGqaCgQJ9//rmsVqsee+wxXXLJJVq3bp3sdrskacGCBYqLi9PChQu1a9cuXX/99Wrbtq0ef/zxaL5dAABaDAI3AAAtXFFRkbxer6688krl5+dLkvr27StJio+Pl9vtVnZ2dmD/t956S36/Xy+//LJMJpMk6bXXXlNqaqoWLlyokSNHSpLsdrteffVVJSQkqHfv3nrkkUd077336tFHH5XZzCQ5AABOhL+WAAC0cP3799fw4cPVt29fXXXVVXrppZd0+PDhY+6/du1abd++XcnJyUpKSlJSUpLS0tJUV1enb775Jui4CQkJge2CggJVVVWpsLAwrO8HAIDWghFuAABaOIvFonnz5um///2v5s6dq2eeeUa/+93vtGzZsqPuX1VVpUGDBulvf/tbo+cyMjLCXS4AADGDwA0AQCtgMpk0bNgwDRs2TA8++KDy8/M1e/Zs2e12+Xy+oH0HDhyod955R5mZmXI6ncc85tq1a1VbW6v4+HhJ0pdffqmkpCTl5eWF9b0AANBaMKUcAIAWbtmyZfrDH/6gFStWaM+ePXrvvfd08OBBnXHGGerYsaPWrVunLVu2qLS0VB6PR+PHj1d6errGjh2rzz//XDt37tTChQt1++23a+/evYHj1tfX68Ybb9TGjRs1Z84cPfTQQ7rtttu4fhsAgJPECDcAAC2c0+nU4sWL9dRTT8nlcik/P19/+tOfNHr0aA0ePFgLFy7U4MGDVVVVpc8++0wXXnihFi9erPvuu09XXnmlKisr1a5dOw0fPjxoxHv48OHq1q2bzj//fLndbo0bN04PP/xw9N4oAAAtjMkwDCPaRQAAgObluuuuU3l5ud5///1olwIAQIvFnDAAAAAAAMKAwA0AAAAAQBgwpRwAAAAAgDBghBsAAAAAgDAgcAMAAAAAEAYEbgAAAAAAwoDADQAAAABAGBC4AQAAAAAIAwI3AAAAAABhQOAGAAAAACAMCNwAAAAAAIQBgRsAAAAAgDD4/wEVnjT6QwEt6gAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# empty prompt to generate random stuff\n",
        "prompt = torch.full((1, llm_config.context_size), tokenizer.pad_id, dtype=torch.int32).to(train_config.device)\n",
        "out = model.generate(prompt, max_seq_len=30)\n",
        "tokenizer.decode(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KCk2yTTLqwl3",
        "outputId": "a333efa2-b78b-49ae-ec11-c70df8837f56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'. ⁇  Yet Tagore he laffirmed his knighthood-ahaji Redasi movement Chil'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from a prompt\n",
        "prompt = (\n",
        "    tokenizer.encode(\n",
        "        \"Ravi's brother Hemendranath tutored and physically\",\n",
        "        beg_of_string=True,\n",
        "        pad_seq=True,\n",
        "        seq_len=llm_config.context_size,\n",
        "    )\n",
        "    .view(1, -1)\n",
        "    .to(train_config.device)\n",
        ")\n",
        "out = model.generate(prompt, max_seq_len=50)\n",
        "tokenizer.decode(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "JAFP2ik9q7OR",
        "outputId": "2e9ca569-6a4f-49fe-9950-2804c6890a2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Ravi's brother Hemendranath tutored and physically conditioned him ⁇ by having him swim the Ganges or trek through hills, by gymnastics, and by practising judo and wrest\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ]
}